name: Electron Microscopy Data Ingestion Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      stage:
        description: 'Pipeline stage to run'
        required: true
        default: 'all'
        type: choice
        options:
          - 'stage1'
          - 'stage2'  
          - 'stage3'
          - 'all'
          - 'consolidate'
      memory_limit:
        description: 'Memory limit (e.g., 8g, 16g)'
        required: false
        default: '8g'

env:
  # CI/CD metadata that will be injected into processing
  CI_COMMIT_SHA: ${{ github.sha }}
  CI_COMMIT_REF: ${{ github.ref_name }}
  CI_PIPELINE_ID: ${{ github.run_id }}
  CI_PIPELINE_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
  CI_TRIGGERED_BY: ${{ github.actor }}
  CI_WORKFLOW_NAME: ${{ github.workflow }}
  
  # Docker memory limits for GitHub Actions (8GB available)
  OPENORGANELLE_MEMORY_LIMIT: ${{ github.event.inputs.memory_limit || '8g' }}
  EPFL_MEMORY_LIMIT: '4g'
  EBI_MEMORY_LIMIT: '1g'
  FLYEM_MEMORY_LIMIT: '1g'
  IDR_MEMORY_LIMIT: '1g'
  CONSOLIDATE_MEMORY_LIMIT: '1g'

jobs:
  # Code Quality: Lint and format checks
  code-quality:
    name: "Code Quality Checks"
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint black isort flake8
          pip install -r requirements.txt
          
      - name: Run Black format check
        run: |
          black --check --diff .
        continue-on-error: true
        
      - name: Run isort import check
        run: |
          isort --check-only --diff .
        continue-on-error: true
        
      - name: Run Flake8 style check
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        continue-on-error: true
        
      - name: Run Pylint analysis
        run: |
          # Create pylint output directory
          mkdir -p pylint-output
          
          # Run pylint on main source directories
          echo "Running Pylint on lib/"
          pylint lib/ --output-format=text --reports=yes --score=yes > pylint-output/lib-report.txt || true
          
          echo "Running Pylint on app/"
          pylint app/ --output-format=text --reports=yes --score=yes > pylint-output/app-report.txt || true
          
          # Generate combined report
          echo "=== Pylint Code Quality Report ===" > pylint-output/combined-report.txt
          echo "Generated on: $(date)" >> pylint-output/combined-report.txt
          echo "Commit: ${{ github.sha }}" >> pylint-output/combined-report.txt
          echo "" >> pylint-output/combined-report.txt
          
          echo "=== LIB DIRECTORY ===" >> pylint-output/combined-report.txt
          cat pylint-output/lib-report.txt >> pylint-output/combined-report.txt
          echo "" >> pylint-output/combined-report.txt
          
          echo "=== APP DIRECTORY ===" >> pylint-output/combined-report.txt
          cat pylint-output/app-report.txt >> pylint-output/combined-report.txt
          
          # Extract scores for summary
          LIB_SCORE=$(grep "Your code has been rated at" pylint-output/lib-report.txt | grep -o '[0-9]\+\.[0-9]\+' | head -1 || echo "N/A")
          APP_SCORE=$(grep "Your code has been rated at" pylint-output/app-report.txt | grep -o '[0-9]\+\.[0-9]\+' | head -1 || echo "N/A")
          
          echo "=== SUMMARY ===" >> pylint-output/combined-report.txt
          echo "Lib Score: $LIB_SCORE/10.0" >> pylint-output/combined-report.txt
          echo "App Score: $APP_SCORE/10.0" >> pylint-output/combined-report.txt
          
          # Display summary
          echo "## Pylint Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Lib Score:** $LIB_SCORE/10.0" >> $GITHUB_STEP_SUMMARY
          echo "- **App Score:** $APP_SCORE/10.0" >> $GITHUB_STEP_SUMMARY
          echo "- **Full report available in artifacts**" >> $GITHUB_STEP_SUMMARY
          
      - name: Upload Pylint results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pylint-reports
          path: pylint-output/
          retention-days: 30
          
      - name: Upload code quality summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-summary
          path: pylint-output/combined-report.txt
          retention-days: 30

  # Stage 1: Light workloads (EBI, FlyEM, IDR)
  stage1:
    name: "Stage 1: Light Sources"
    runs-on: ubuntu-latest
    needs: [code-quality]
    if: ${{ github.event.inputs.stage == 'stage1' || github.event.inputs.stage == 'all' || github.event.inputs.stage == '' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Create data directories
        run: |
          mkdir -p data/{ebi,flyem,idr}
          
      - name: Run EBI ingestion
        run: |
          docker compose --profile stage1 up ebi --abort-on-container-exit
        env:
          EM_CI_METADATA: 'true'
          
      - name: Run FlyEM ingestion  
        run: |
          docker compose --profile stage1 up flyem --abort-on-container-exit
        env:
          EM_CI_METADATA: 'true'
          
      - name: Run IDR ingestion
        run: |
          docker compose --profile stage1 up idr --abort-on-container-exit
        env:
          EM_CI_METADATA: 'true'
          
      - name: Upload Stage 1 artifacts
        uses: actions/upload-artifact@v4
        with:
          name: stage1-data
          path: |
            data/ebi/
            data/flyem/
            data/idr/
            logs/
          retention-days: 7
          
      - name: Upload Stage 1 logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: stage1-logs
          path: logs/
          retention-days: 30

  # Stage 2: Heavy workload (OpenOrganelle)
  stage2:
    name: "Stage 2: OpenOrganelle"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.stage == 'stage2' || github.event.inputs.stage == 'all' || github.event.inputs.stage == '' }}
    needs: [stage1]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Create data directories
        run: |
          mkdir -p data/openorganelle
          
      - name: Run OpenOrganelle ingestion
        run: |
          docker compose --profile stage2 up openorganelle --abort-on-container-exit
        env:
          EM_CI_METADATA: 'true'
          ZARR_CHUNK_SIZE_MB: '128'
          MAX_WORKERS: '1'
          
      - name: Upload Stage 2 artifacts
        uses: actions/upload-artifact@v4
        with:
          name: stage2-data
          path: |
            data/openorganelle/
            logs/
          retention-days: 7
          
      - name: Upload Stage 2 logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: stage2-logs
          path: logs/
          retention-days: 30

  # Stage 3: Memory intensive (EPFL) 
  stage3:
    name: "Stage 3: EPFL"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.stage == 'stage3' || github.event.inputs.stage == 'all' || github.event.inputs.stage == '' }}
    needs: [stage2]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Create data directories
        run: |
          mkdir -p data/epfl
          
      - name: Run EPFL ingestion
        run: |
          docker compose --profile stage3 up epfl --abort-on-container-exit
        env:
          EM_CI_METADATA: 'true'
          
      - name: Upload Stage 3 artifacts
        uses: actions/upload-artifact@v4
        with:
          name: stage3-data
          path: |
            data/epfl/
            logs/
          retention-days: 7

  # Consolidation: Aggregate all metadata
  consolidate:
    name: "Consolidate Metadata"
    runs-on: ubuntu-latest
    if: always()
    needs: [stage1, stage2, stage3]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*-data"
          merge-multiple: true
          path: .
          
      - name: Run metadata consolidation
        run: |
          docker compose --profile consolidate up consolidate --abort-on-container-exit
        env:
          EM_CI_METADATA: 'true'
          
      - name: Upload consolidated metadata
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-metadata
          path: |
            app/consolidate/metadata_catalog_*.json
            app/consolidate/validation_report_*.json
            app/consolidate/*.log
          retention-days: 30
          
      - name: Generate pipeline summary
        run: |
          echo "## Pipeline Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Pipeline URL:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Artifacts Generated" >> $GITHUB_STEP_SUMMARY
          echo "- Consolidated metadata catalog" >> $GITHUB_STEP_SUMMARY  
          echo "- Validation reports" >> $GITHUB_STEP_SUMMARY
          echo "- Processing logs for all stages" >> $GITHUB_STEP_SUMMARY
          echo "- Processed EM data volumes" >> $GITHUB_STEP_SUMMARY

  # Optional: Run tests after pipeline
  test:
    name: "Run Tests"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.stage == 'all' || github.event.inputs.stage == '' }}
    needs: [consolidate]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Download test data
        uses: actions/download-artifact@v4
        with:
          pattern: "*-data"
          merge-multiple: true
          path: .
          
      - name: Run integration tests
        run: |
          chmod +x scripts/run_tests.sh
          scripts/run_tests.sh
          
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test_results/
            pytest.xml
          retention-days: 30